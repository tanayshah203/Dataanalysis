{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Vehicle Loan Prediction Machine Learning Model\n",
    "\n",
    "# Chapter 4 - Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap and Load\n",
    "\n",
    "- As always, let's begin by importing our libraries and loading the data\n",
    "- Notice that we have a new import from sklearn.preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = pd.read_csv('../data/vehicle_loans_eda.csv', index_col='UNIQUEID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "source": [
    "Let’s bring in the explore_categorical and explore_continuous functions we created in chapter 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_categorical(col_name):   \n",
    "    print(\"{0} Summary\".format(col_name))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"{0} Counts\".format(col_name))\n",
    "    print(loan_df[col_name].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"{0} Ratio\".format(col_name))\n",
    "    print(loan_df[col_name].value_counts(normalize=True))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"{0} Default Counts\".format(col_name))\n",
    "    print(loan_df.groupby(col_name)['LOAN_DEFAULT'].value_counts().unstack(level=-1))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"{0} Default Ratio\".format(col_name))\n",
    "    print(loan_df.groupby(col_name)['LOAN_DEFAULT'].value_counts(normalize=True).unstack(level=-1))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    sns.catplot(data=loan_df,kind='count',x=col_name,hue='LOAN_DEFAULT')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_continuous(col_name):\n",
    "    #print statistical summary\n",
    "    print(\"{0} Summary\".format(col_name))\n",
    "    print(\"\\n\")\n",
    "    print(loan_df[col_name].describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #Look at boxplot\n",
    "    sns.boxplot(x=col_name, data=loan_df)\n",
    "    plt.show()\n",
    "\n",
    "    #Look at the distribution\n",
    "    sns.distplot(loan_df[col_name], hist=False)\n",
    "    plt.show()\n",
    "\n",
    "    #Now lets look deeper by grouping with the target variable \n",
    "    print(\"{0} Grouped Summary\".format(col_name))\n",
    "    print(\"\\n\")\n",
    "    print(loan_df.groupby('LOAN_DEFAULT')[col_name].describe())\n",
    "\n",
    "    #look at grouped boxplot \n",
    "    sns.boxplot(x=col_name, y='LOAN_DEFAULT', data=loan_df, orient=\"h\")\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "## Lesson 1 - Binning\n",
    "\n",
    "Our first lesson is all about binning, so make sure you have covered the theory lesson that accompanies this chapter\n",
    "\n",
    "In our exploratory analysis, we observed that disbursed amount had some extremely large values?\n",
    "\n",
    "Let's take another look at that with our explore_continuous function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "explore_continuous('DISBURSED_AMOUNT')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Notice that the largest value here is 990,572 this is considerably larger than the next largest value which is also an extreme outlier \n",
    "\n",
    "Is it possible that this could be an error in the data?\n",
    "- If we assume that the disbursal amount is stored in Indian rupees then 990,572 equates to ~ £10,000 or $13,400\n",
    "- Indeed a lot of money but not impossible for a car loan \n",
    "\n",
    "Let's check the row data, LTV and ASSET_COST should give us an idea about whether or not this is a valid piece of data \n",
    "\n",
    "Pandas gives us the [idxmax](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.idxmax.html) function to find the index of the row with the maximum value of a column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['DISBURSED_AMOUNT'].idxmax()"
   ]
  },
  {
   "source": [
    "Now we have the row index but how can we select the row?\n",
    "\n",
    "Dataframe rows can be selected by their index using the [loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.loc[loan_df['DISBURSED_AMOUNT'].idxmax()]"
   ]
  },
  {
   "source": [
    "Ok, it seems reasonable that the applicant borrowed 990,572 to pay for a car that cost 1,628,992. Let's assume a legitimate value that we want to include in our model \n",
    "\n",
    "One way we can keep extreme values without them skewing the data is through binning\n",
    "- Creating categorical groups from continuous variables \n",
    "\n",
    "Let's create some bins based on the disbursed amount\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "disbursed_buckets = [13000, 30000, 45000, 60000, 75000, 150000, 1000000]\n",
    "disbursed_labels = ['13k - 30k', '30k - 45k', '45k - 60k', '60k - 75k', '75k - 150k', '150k - 1m']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Great, now we need to use these bins to categorize our rows based on their disbursed amounts\n",
    "\n",
    "Pandas provides the [cut](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) which allows us to create buckets and labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['DISBURSED_CAT'] = pd.cut(loan_df['DISBURSED_AMOUNT'], disbursed_buckets, labels=disbursed_labels)"
   ]
  },
  {
   "source": [
    "Let's use explore_categorical to take a look at our new column!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_categorical('DISBURSED_CAT')"
   ]
  },
  {
   "source": [
    "Notice that the vast majority of loans fall in the '45k - 60k' bucket. \n",
    "\n",
    "Also, the higher the loan amount, the more likely it was to default except for loans in the very largest category"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 2 - Combine Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Disbursal Difference\n",
    "\n",
    "One brilliant feature of pandas, is that it allows us to easily create new columns by coming existing ones \n",
    "\n",
    "We can use mathematical operations to create new features from existing ones with just one line of code! \n",
    "\n",
    "- HINT: We already did this to create our AGE column in chapter 2\n",
    "\n",
    "Let's revisit this idea by creating a DISBURSAL_DIFFERENCE column which will be the difference between the ASSET_COST and the DISBURSED_AMOUNT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['DISBURSAL_DIFFERENCE'] = loan_df['ASSET_COST'] - loan_df['DISBURSED_AMOUNT']"
   ]
  },
  {
   "source": [
    "Let's inspect our new column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df[['DISBURSAL_DIFFERENCE', 'ASSET_COST', 'DISBURSED_AMOUNT']].sample(5)"
   ]
  },
  {
   "source": [
    "### Primary and Secondary Account Engineering\n",
    "\n",
    "The loan data contains 14 columns providing information about any existing loans a customer may have. This data is split according to primary and secondary accounts, a primary account is a loan where the customer is the sole recipient of the disbursal, \n",
    "a secondary account is one where the customer is a co-applicant or guarantor for the loan \n",
    "\n",
    "- PRI_NO_OF_ACCTS: count of total loans taken by the customer at the time of disbursement  \n",
    "- PRI_ACTIVE_ACCTS: count of active loans taken by the customer at the time of disbursement\n",
    "- PRI_OVERDUE_ACCTS: count of default accounts at the time of disbursement    \n",
    "- PRI_CURRENT_BALANCE: total Principal outstanding amount of the active loans at the time of disbursement        \n",
    "- PRI_SANCTIONED_AMOUNT: total amount that was sanctioned for all the loans at the time of disbursement  \n",
    "- PRI_DISBURSED_AMOUNT: total amount that was disbursed for all the loans at the time of disbursement\n",
    "- PRIMARY_INSTAL_AMT: EMI Amount of the primary loan\n",
    "- SEC_NO_OF_ACCTS: count of secondary total loans taken by the customer at the time of disbursement \n",
    "- SEC_ACTIVE_ACCTS: count of secondary active loans taken by the customer at the time of disbursement \n",
    "- SEC_OVERDUE_ACCTS: count of secondary accounts at the time of disbursement    \n",
    "- SEC_CURRENT_BALANCE: total Principal outstanding amount of the secondary active loans at the time of disbursement \n",
    "- SEC_SANCTIONED_AMOUNT: total amount that was sanctioned for all the secondary loans at the time of disbursement  \n",
    "- SEC_DISBURSED_AMOUNT: total amount that was disbursed for all the secondary loans at the time of disbursement\n",
    "- SEC_INSTAL_AMT: EMI Amount of the secondary loan \n",
    "\n",
    "If you haven’t done so already please take a few minutes to explore these primary and secondary account variables using the explore_continuous function\n",
    "\n",
    "Hopefully, you will see that in the majority of cases the data in these columns is limited\n",
    "\n",
    "Instead of simply dropping them, we will combine so we can reduce the complexity of our data without losing information\n",
    "\n",
    "For example, we can create a column 'TOTAL_ACCTS' to store the combined total of primary and secondary accounts\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['TOTAL_ACCTS'] = loan_df['PRI_NO_OF_ACCTS'] + loan_df['SEC_NO_OF_ACCTS']"
   ]
  },
  {
   "source": [
    "Let's look at the results of this just to sanity check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df[['TOTAL_ACCTS', 'PRI_NO_OF_ACCTS', 'SEC_NO_OF_ACCTS']].sample(10)"
   ]
  },
  {
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Use this example to create TOTAL_ columns from the remaining primary/secondary variables\n",
    "- HINT: After this, you should have 7 new columns including 'TOTAL_ACCTS'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### SOLUTION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['TOTAL_ACTIVE_ACCTS'] = loan_df['PRI_ACTIVE_ACCTS'] + loan_df['SEC_ACTIVE_ACCTS']\n",
    "loan_df['TOTAL_OVERDUE_ACCTS'] = loan_df['PRI_OVERDUE_ACCTS'] + loan_df['SEC_OVERDUE_ACCTS']\n",
    "loan_df['TOTAL_CURRENT_BALANCE'] = loan_df['PRI_CURRENT_BALANCE'] + loan_df['SEC_CURRENT_BALANCE']\n",
    "loan_df['TOTAL_SANCTIONED_AMOUNT'] = loan_df['PRI_SANCTIONED_AMOUNT'] + loan_df['SEC_SANCTIONED_AMOUNT'] \n",
    "loan_df['TOTAL_DISBURSED_AMOUNT'] = loan_df['PRI_DISBURSED_AMOUNT'] + loan_df['SEC_DISBURSED_AMOUNT']\n",
    "loan_df['TOTAL_INSTAL_AMT'] = loan_df['PRIMARY_INSTAL_AMT'] + loan_df['SEC_INSTAL_AMT']"
   ]
  },
  {
   "source": [
    "Brilliant! Now we have simplified our dataset by combining the primary and secondary account columns \n",
    "\n",
    "Let's drop the ones we no longer need to avoid data duplication"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT', 'SEC_INSTAL_AMT']\n",
    "\n",
    "loan_df = loan_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "source": [
    "We may also be interested in calculating the percentage of overdue accounts a person has"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### EXERCISE\n",
    "\n",
    "- Create a column 'OVERDUE_PCT' that stores the percentage of overdue accounts\n",
    "\n",
    "### SOLUTION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['OVERDUE_PCT'] = loan_df['TOTAL_OVERDUE_ACCTS'] / loan_df['TOTAL_ACCTS']"
   ]
  },
  {
   "source": [
    "Wait! We better check this new column for missing values, remember a lot of rows will have 0 'TOTAL_ACCTS'\n",
    "\n",
    "We can investigate using [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isnull.html) and [sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sum.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['OVERDUE_PCT'].isnull().sum()"
   ]
  },
  {
   "source": [
    "Ok, we have a lot of missing values we can fill them with 0s using [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['OVERDUE_PCT'] = loan_df['OVERDUE_PCT'].fillna(0)"
   ]
  },
  {
   "source": [
    "Let's use [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isnull.html) and [sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sum.html) again to make sure we no longer have missing values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['OVERDUE_PCT'].isnull().sum()"
   ]
  },
  {
   "source": [
    "Brilliant, our new OVERDUE_PCT column no longer has any missing values. \n",
    "\n",
    "We have done a lot of feature engineering so far, so let's use the [info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) function to take a look at our columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.info()"
   ]
  },
  {
   "source": [
    "Nice! We have reduced our number of columns to 30\n",
    "\n",
    "Also, notice the presence of our new columns at the bottom of the list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lesson 3 - Scaling \n",
    "\n",
    "- We will use min max scaling to bring our continuous variables into the same range \n",
    "- Outliers have been observed in the data. However, we will keep them as they appear to be legitimate\n",
    "- Sklearn provides an implementation of Min Max scaling, [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "First, we create a list of our continuous columns \n",
    "- We can use this list to select the continuous variables from our dataframe without typing a long list of column names each time \n",
    "- We don't need to scale our categoricals or binaries\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['DISBURSED_AMOUNT', \n",
    "                'ASSET_COST', \n",
    "                'LTV', \n",
    "                'NEW_ACCTS_IN_LAST_SIX_MONTHS', \n",
    "                'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n",
    "                'NO_OF_INQUIRIES', \n",
    "                'AGE', \n",
    "                'AVERAGE_ACCT_AGE_MONTHS', \n",
    "                'CREDIT_HISTORY_LENGTH_MONTHS',\n",
    "                'PERFORM_CNS_SCORE',\n",
    "                'TOTAL_ACCTS',\n",
    "                'TOTAL_ACTIVE_ACCTS',\n",
    "                'TOTAL_OVERDUE_ACCTS',\n",
    "                'TOTAL_CURRENT_BALANCE', \n",
    "                'TOTAL_SANCTIONED_AMOUNT', \n",
    "                'TOTAL_DISBURSED_AMOUNT', \n",
    "                'TOTAL_INSTAL_AMT', \n",
    "                'OVERDUE_PCT', \n",
    "                'DISBURSAL_DIFFERENCE']"
   ]
  },
  {
   "source": [
    "Let's look at the numeric columns so we have a reference. \n",
    "\n",
    "We can plot multiple columns on the same plot using pandas [boxplot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df[['LTV', 'ASSET_COST', 'AGE']].boxplot()\n",
    "plt.title('Prior to Scaling')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Observe that the numeric values are distributed across very different scales.\n",
    "\n",
    "Let's fix this problem using Min Max Scaling. \n",
    "\n",
    "First we need to create an instance of [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler()"
   ]
  },
  {
   "source": [
    "We can use [fit_transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit_transform) to fit the scaler to our data and perform the scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df[numeric_cols] = mm_scaler.fit_transform(loan_df[numeric_cols])"
   ]
  },
  {
   "source": [
    "Let's have a quick look at the boxplots now"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df[['LTV', 'ASSET_COST', 'AGE']].boxplot()\n",
    "plt.title('After Scaling')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Now all the values are ranged between 1 and 0\n",
    "\n",
    "Take some time now to look at some of our other numeric columns after we have scaled them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "- In this chapter, we have performed some basic feature engineering \n",
    "- We have created a new categorical column using binning on 'DISBURSED_AMOUNT'\n",
    "- We have combined features to create new ones \n",
    "- We have performed MinMax scaling on our numeric variables\n",
    "\n",
    "Let's save our data to file!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.to_csv('../data/vehicle_loans_feat.csv')"
   ]
  }
 ]
}