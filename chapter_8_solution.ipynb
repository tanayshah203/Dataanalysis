{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8-final"},"colab":{"name":"chapter_8_solution.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"37vz6vwPYode"},"source":["# Vehicle Loan Prediction Machine Learning Model\n","\n","# Chapter 8 - Class Balancing\n","\n","### Recap and Load\n","- As always, let's begin by importing our libraries and loading the data\n","- Notice that we are importing SMOTE from imblearn.over_sampling and resample from sklearn.utils\n","\n","*Throughout this chapter you may see slightly different results to those on the demo videos. The outputs vary due to the random nature of the random forest algorithm and some of the sampling we will use but they should be similar to those in the videos*\n","\n","*Some of the models we will build here are a bit more complex, if you are running into memory related issues try and free up memory by closing down any programs that you do not need to complete the chapter*"]},{"cell_type":"markdown","metadata":{"id":"4e-Vh7KSonR4"},"source":["##### If you do not have the imblearn package installed then you should install it by running \"pip install imblearn\" from the command line"]},{"cell_type":"code","metadata":{"id":"14F_mVIIZEkL"},"source":["!pip install imblearn "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ofQjFfqYodg"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score, roc_curve, auc, precision_score, plot_confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from sklearn.utils import resample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55pzVDjYYodh"},"source":["loan_df = pd.read_csv('../data/vehicle_loans_feat.csv', index_col='UNIQUEID')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gohH00lJYodi"},"source":["category_cols = ['MANUFACTURER_ID', 'STATE_ID', 'DISBURSAL_MONTH', 'DISBURSED_CAT', 'PERFORM_CNS_SCORE_DESCRIPTION', 'EMPLOYMENT_TYPE']\n","loan_df[category_cols] = loan_df[category_cols].astype('category')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjvMnQWOYodi"},"source":["def plot_roc_curve(fpr, tpr, roc_auc):\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTdCn8C9Yodi"},"source":["def eval_model(model, x_test, y_test):\n","    preds = model.predict(x_test)\n","    probs = model.predict_proba(x_test)\n","\n","    conf_mat = confusion_matrix(y_test, preds)\n","    accuracy = accuracy_score(y_test, preds)\n","    recall = recall_score(y_test, preds)\n","    precision = precision_score(y_test, preds)\n","    f1 = f1_score(y_test, preds)\n","\n","    plot_confusion_matrix(model, x_test, y_test)\n","    plt.show()\n","\n","    #print(conf_mat)\n","    print(\"\\n\")\n","    print(\"Accuracy: \", accuracy)\n","    print(\"Precision: \", precision)\n","    print(\"Recall: \", recall)\n","    print(\"F1: \", f1)\n","\n","    #Show ROC Curve \n","    fpr, tpr, threshold = roc_curve(y_test, probs[:,1], pos_label=1)\n","    roc_auc = auc(fpr, tpr)\n","    print(\"AUC: \", roc_auc)\n","\n","    plot_roc_curve(fpr, tpr, roc_auc)\n","\n","    results_df = pd.DataFrame()\n","    results_df['true_class'] = y_test\n","    results_df['predicted_class'] = list(preds)\n","    results_df['default_prob'] = probs[:, 1]\n","\n","    #plot the distribution of probabilities for the estimated classes \n","    sns.distplot(results_df[results_df['true_class'] == 0]['default_prob'], label=\"No Default\", hist=False)\n","    sns.distplot(results_df[results_df['true_class'] == 1]['default_prob'], label=\"Default\", hist=False)\n","    plt.title('Distribution of Probabilities for Estimated Classes')\n","    plt.legend(loc='best')\n","    plt.show()\n","    \n","    #see the true class versus predicted class as a percentage\n","    print(results_df.groupby('true_class')['predicted_class'].value_counts(normalize=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvgWrED1Yodj"},"source":["def encode_and_split(loan_df):\n","    loan_data_dumm = pd.get_dummies(loan_df, prefix_sep='_', drop_first=True)\n","\n","    x = loan_data_dumm.drop(['LOAN_DEFAULT'], axis=1)\n","    y = loan_data_dumm['LOAN_DEFAULT']\n","\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","    return x_train, x_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YEJ7es2IYodj"},"source":["## Lesson 1 - Weight Balancing \n","\n","### Random Forest \n","\n","- Just for fun let's build and train the random forest classifier with the parameters we found in chapter 7\n","\n","### EXERCISE \n","\n","- Create training and test data for the full feature set \n","- Use the tuned hyperparameters from the previous chapter to create a baseline Random Forest\n"]},{"cell_type":"markdown","metadata":{"id":"fM5R1FOlYodk"},"source":["### SOLUTION"]},{"cell_type":"code","metadata":{"id":"SbmAUonTYodk"},"source":["x_train, x_test, y_train, y_test = encode_and_split(loan_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1iExwKdYodl"},"source":["rfc_model = RandomForestClassifier(n_estimators = 100, max_depth = 15)\n","\n","rfc_model.fit(x_train, y_train)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VqQa0NSYodl"},"source":["\n","- This model is the best we have seen so far at separating the two classes, but it is still not managing to identify many defaults\n","- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) has a class_weight parameter that can be used to address the problem of class imbalance \n","- By setting class_weight to 'balanced', we tell sklearn to automatically adjust the weights for each class so that they have equal influence during model training\n"]},{"cell_type":"code","metadata":{"id":"k5T2ooVJYodl"},"source":["rfc_model = RandomForestClassifier(n_estimators = 100, max_depth = 15, class_weight='balanced')\n","\n","rfc_model.fit(x_train, y_train)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XX77SE7yYodm"},"source":["Some big changes to the model performance\n","\n","### Accuracy \n","\n","- Dropped to ~63%\n","\n","### Precision \n","\n","- 30% down from 66% for the random forest with default class weighting\n","- Less of the instances we classified as defaults actually were defaults \n","\n","### Recall \n","\n","- Recall has increased dramatically, from 0.5% to 53%!\n","- We are now able to correctly identify over half of the loan defaults\n","- However, we are also incorrectly classifying a large number of non-defaults as defaults\n","\n","### F1\n","\n","- The F1 score has also increased dramatically from 0.01 to 38 \n","- We have a much more balanced model than we did previously\n","\n","### AUC \n","\n","- The area under the roc curve has dropped from 0.65 to 0.64\n","\n","### Probability Distributions \n","\n","- THe main peak for Non defaults is inline with the peak for defaults, this makes sense as we know the model is classifying a lot of non-defaults as defaults\n","\n","Generally, this is a better, more balanced model than the previous ones. However the large numbers of non-defaults predicted as defaults is concerning"]},{"cell_type":"markdown","metadata":{"id":"UfRLH28JYodm"},"source":["### Manual Class Weighting\n","\n","Passing 'balanced' to the class weight parameter adjusts the class weights so that both classes carry equal influence \n","\n","We can also pass a dictionary to class weights to manually set class weightings \n","\n","- default (no class weighting) is equivalent to class_weight = {0:0.5, 1:0.5}\n","- In our case 'balanced' is equivalent to class_weight = {0:0.217, 1:0.783}\n","- With balanced classes we are incorrectly predicting too many defaults\n","- Let's tweak the class weights to see if we can amend this"]},{"cell_type":"code","metadata":{"id":"dg27oFl_Yodn"},"source":["weights = {0:0.27, 1:0.73}\n","\n","rfc_model = RandomForestClassifier(n_estimators = 100, max_depth = 15, class_weight=weights)\n","\n","rfc_model.fit(x_train, y_train)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTLodufkYodo"},"source":["We have increased the precision and accuracy, but the recall has dropped dramatically!\n","\n","Hopefully, you are starting to see that the process of building and tweaking models is iterative and often there is no definitive best answer, just alternatives"]},{"cell_type":"markdown","metadata":{"id":"_Rs7Aq0DYodo"},"source":["## Lesson 2 - Resampling\n","\n","- We will look at Upsampling, Downsampling and SMOTE (Synthetic Minority Oversampling)\n","- IMPORTANT - Resampling should only be done on the training data, test data should always reflect the 'natural class distribution'\n","\n","### Upsampling\n","\n","- Upsampling is the process of resampling the minority class to match the number of instances in the majority\n","- We only want to resample the training data so let's join the x_train and y_train into one DataFrame\n","- We create a [copy](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html) of x_train to avoid the need to recreate our train test split later"]},{"cell_type":"code","metadata":{"id":"MBGLjzWIYodo"},"source":["train_df = x_train.copy()\n","train_df['LOAN_DEFAULT'] = y_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JXtnI9xZYodp"},"source":["- Let's make sure we know what the class distribution looks like "]},{"cell_type":"code","metadata":{"id":"9IemsGhAYodp"},"source":["print(train_df['LOAN_DEFAULT'].value_counts())\n","print(train_df['LOAN_DEFAULT'].value_counts(normalize=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJPhTNU9Yodp"},"source":["- Class distribution is what we expected\n","- Now we can create subsets for each class value"]},{"cell_type":"code","metadata":{"id":"eU6MkTWIYodp"},"source":["train_minority = train_df[train_df['LOAN_DEFAULT'] == 1]\n","train_majority = train_df[train_df['LOAN_DEFAULT'] == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRuWvFrzYodp"},"source":["sklearn provides us with the [resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) function\n","\n","we pass it 4 parameters \n","- the dataframe which we want to resample\n","- replace: boolean indicating that we want to resample with replacement\n","- n_samples: the number of samples to be created \n","- random_state: random seed for repeatability"]},{"cell_type":"code","metadata":{"id":"6jJ22gnmYodq"},"source":["train_minority_up = resample(train_minority, replace=True,  n_samples=train_majority.shape[0], random_state=123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aut1rJOnYodq"},"source":["We should now have two DataFrames, train_majority and train_minority_up. These should both have the same number of rows\n","- all the instances in train_majority should be of class 0\n","- all the instances in train_minority_up should be of class 1"]},{"cell_type":"code","metadata":{"id":"-rkWSlQGYodq"},"source":["print(train_majority['LOAN_DEFAULT'].value_counts())\n","print(train_minority_up['LOAN_DEFAULT'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kRwiVbWxYodq"},"source":["Nice! now we can use the [concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) function from pandas to join the two dataframes together"]},{"cell_type":"code","metadata":{"id":"R0iK5KkcYodq"},"source":["train_up_df = pd.concat([train_majority, train_minority_up])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLZvYLbiYodr"},"source":["Let's check the class distribution, it should be 50/50"]},{"cell_type":"code","metadata":{"id":"bhjVkpeGYodr"},"source":["print(train_up_df['LOAN_DEFAULT'].value_counts())\n","print(train_up_df['LOAN_DEFAULT'].value_counts(normalize=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJD2F-riYodr"},"source":["Finally, we need to separate the target variable from our features and then re-train/evaluate the model"]},{"cell_type":"code","metadata":{"id":"GCv0MP8NYodr"},"source":["x_train_up = train_up_df.drop(['LOAN_DEFAULT'], axis=1)\n","y_train_up = train_up_df['LOAN_DEFAULT']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arQcqWoiYodr"},"source":["rfc_model = RandomForestClassifier(n_estimators = 100, max_depth=15)\n","\n","rfc_model.fit(x_train_up, y_train_up)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4mWSuGIYodr"},"source":["Almost identical results to using 'balanced' class weights. This makes sense if you think about it! "]},{"cell_type":"markdown","metadata":{"id":"V9VBGEHgYodr"},"source":["### Downsampling\n","\n","- Produce balanced training data by downsampling the majority class to match the minority\n","- We have provided a function balance subsample to help you easily perform up or downsampling!"]},{"cell_type":"code","metadata":{"id":"ohHy68JrYods"},"source":["def balance_sample(x_train, y_train, sample_mode='up'):\n","    train_df = x_train.copy()\n","    train_df['LOAN_DEFAULT'] = y_train\n","\n","    train_minority = train_df[train_df['LOAN_DEFAULT'] == 1]\n","    train_majority = train_df[train_df['LOAN_DEFAULT'] == 0]\n","\n","    train_sampled_df = pd.DataFrame()\n","\n","    if sample_mode == 'down':\n","        train_majority_down = resample(train_majority, replace=False,  n_samples=train_minority.shape[0], random_state=123)\n","        train_sampled_df = pd.concat([train_minority, train_majority_down])  \n","    else:\n","        train_minority_up = resample(train_minority, replace=True,  n_samples=train_majority.shape[0], random_state=123)\n","        train_sampled_df = pd.concat([train_majority, train_minority_up])\n","\n","    x_train_samp = train_sampled_df.drop(['LOAN_DEFAULT'], axis=1)\n","    y_train_samp = train_sampled_df['LOAN_DEFAULT']\n","\n","    return x_train_samp, y_train_samp "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-QwnuRlYods"},"source":["How about downsampling on the Random Forest\n","\n","### EXERCISE\n","\n","- Use the balance_sample function to create downsampled training data\n","- HINT: look at the 'sample_mode' parameter\n","\n","### SOLUTION"]},{"cell_type":"code","metadata":{"id":"lRu0c2FQYods"},"source":["#downsample random forest\n","x_train_dwn, y_train_dwn = balance_sample(x_train, y_train, sample_mode='down')\n","\n","print(y_train_dwn.value_counts())\n","print(y_train_dwn.value_counts(normalize=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2mfeHlsYods"},"source":["Nice, lets train and evaluate a RandomForestClassifier using the downsampled training data"]},{"cell_type":"code","metadata":{"id":"_IpxrqVUYods"},"source":["rfc_model = RandomForestClassifier(n_estimators = 100, max_depth = 15)\n","\n","rfc_model.fit(x_train_dwn, y_train_dwn)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-valSKoyYods"},"source":["### SMOTE\n","\n","- [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) upsamples the minority class \n","- Rather than simply duplicating data, SMOTE creates new synthetic instances\n","\n","Let's use SMOTE to create a synthetically upsampled training set\n","\n","First initialize smote"]},{"cell_type":"code","metadata":{"id":"NYW5jiwyYods"},"source":["smote = SMOTE()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aGYcgo0Yodt"},"source":["Now we create the training data using fit_resample"]},{"cell_type":"code","metadata":{"id":"2DD7GumeYodt"},"source":["x_train_synth, y_train_synth = smote.fit_resample(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g_5A_6fVlP_Q"},"source":["*NB - If SMOTE takes too long to run or causes a memory error on your machine please uncomment and run the code in the cell below to use the pre-sampled training data we have provided*"]},{"cell_type":"code","metadata":{"id":"0gIVB4pVlTwc"},"source":["# vehicle_train_synth = pd.read_csv('../data/vehicle_train_synth.csv')\n","\n","# x_train_synth = vehicle_train_synth.drop(['LOAN_DEFAULT'], axis=1)\n","# y_train_synth = vehicle_train_synth['LOAN_DEFAULT']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aneyusw6Yodt"},"source":["Double check the class distribution"]},{"cell_type":"code","metadata":{"id":"65FLnIkkYodt"},"source":["print(y_train_synth.value_counts())\n","print(y_train_synth.value_counts(normalize=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7TxD0snYodt"},"source":["Now train and evaluate the model with the SMOTE sampled training data"]},{"cell_type":"code","metadata":{"id":"r0rWgpT1Yodt"},"source":["rfc_model = RandomForestClassifier(n_estimators = 100, max_depth = 15)\n","\n","rfc_model.fit(x_train_synth, y_train_synth)\n","eval_model(rfc_model, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjb1iowoYodt"},"source":["- Using SMOTE we have trained a model which labels a lot of our test data with the default class\n","- We again have quite poor precision around 30%\n","- The AUC is down to ~0.62\n","- And 57% of our defaulted loans were incorrectly labelled as non-defaults!"]},{"cell_type":"markdown","metadata":{"id":"5fVxMGMPYodt"},"source":["## Conclusion\n","\n","- Well done for making it to the end of the course! \n","- Throughout this course, we have demonstrated how to preprocess data, build models and optimize those models \n","- Loan default prediction is difficult, in the real world large teams of experienced data scientists work on these problems every day\n","- The main thing you should takeaway from this course is practical skills that will allow you to perform preprocessing and predictive modelling on your own datasets "]}]}