{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Loan Prediction Machine Learning Model\n",
    "\n",
    "# Chapter 6 - Model Evaluation\n",
    "\n",
    "## Lesson 1 - Introduction \n",
    "\n",
    "- Before we get started let's load the libraries we want \n",
    "- Notice the addition of sklean.metrics and scikitplot.metrics\n",
    "- If you see an error later in the chapter referring to plot_confusion_matrix then you may need to upgrade to the latest version of sklearn by running \"pip install --upgrade scikit-learn\" from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score, roc_curve, auc, precision_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we should load our data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = pd.read_csv('../data/vehicle_loans_feat.csv', index_col='UNIQUEID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "- Can you build the same simple logistic regression model we created in the previous chapter \n",
    "- HINT: Remember to convert your categorical data to the 'category' type\n",
    "- HINT: Remember to train your model using [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit)\n",
    "- If you find it easier you can add additional code cells to breakdown the exercise into smaller chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['MANUFACTURER_ID', 'STATE_ID', 'DISBURSAL_MONTH', 'DISBURSED_CAT', 'PERFORM_CNS_SCORE_DESCRIPTION', 'EMPLOYMENT_TYPE']\n",
    "loan_df[category_cols] = loan_df[category_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cols = ['STATE_ID', 'LTV', 'DISBURSED_CAT', 'PERFORM_CNS_SCORE', 'DISBURSAL_MONTH', 'LOAN_DEFAULT']\n",
    "loan_df_sml = loan_df[small_cols]\n",
    "\n",
    "loan_data_dumm = pd.get_dummies(loan_df_sml, prefix_sep='_', drop_first=True)\n",
    "\n",
    "x = loan_data_dumm.drop(['LOAN_DEFAULT'], axis=1)\n",
    "y = loan_data_dumm['LOAN_DEFAULT']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now let's use [score](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) to get the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Evaluation Metrics\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "- We can create a simple confusion matrix by combining the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) functions from sklearn\n",
    "\n",
    "- First, we need use [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) to extract our model's predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logistic_model.predict(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have already seen that [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) returns an array of the predicted classes\n",
    "- Now we can pass it into the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) method alongside the real classes for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, preds)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) returns a 2d array of TN, FP, FN and TP\n",
    "\n",
    "2d array is an array with an array, we can access the elements using indexes much like a regular array \n",
    "\n",
    "In our case, we can think of the confusion_matrix as a table with rows and columns \n",
    "\n",
    "we access an element\\[row_idx\\]\\[col_idx\\]\n",
    "\n",
    "Let's extract tn and fp from the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = conf_mat[0][0]\n",
    "fp = conf_mat[0][1]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's print the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Negatives (Correct Non-Defaults): \", tn)\n",
    "print(\"False Positives (Incorrect Defaults): \", fp)\n",
    "print(\"False Negatives (Incorrect Non-Defaults): \", fn)\n",
    "print(\"True Positives (Correct Defaults)\", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now the 78% accuracy makes a bit more sense, our model is pretty much predicting all loans as non-defaults\n",
    "\n",
    "- Remember from previous lessons that about 78% of loans in our dataset were non-defaults. \n",
    "- This is a good example of where accuracy can be a misleading statistic. \n",
    "- Reporting that you predicted 78% correctly might appear impressive, but in reality, the task is to identify loans that will default, from this perspective the model has failed completely \n",
    "\n",
    "sklearn also provides a useful function for generating plots of confusion matrices [plot_confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)\n",
    "\n",
    "It takes 3 required parameters \n",
    "- the trained model\n",
    "- the test features \n",
    "- the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logistic_model, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you will have gathered that accuracy alone is not a reliable indicator of model performance let's discuss some other measures of performance \n",
    "\n",
    "### Precision  = TP/TP + FP \n",
    "\n",
    "- Out of those we predicted as positive, how many actually were positive?\n",
    "- Useful when the cost of false positives is high. i.e. in an email spam filter \n",
    "\n",
    "sklearn provides the [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) function which takes 2 parameters \n",
    "- the test labels \n",
    "- the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, preds)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, out of the positive predictions, 75% were actually positive. Although, we can see from the confusion matrix that the model  only predicted a small number of instances as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall = TP/TP + FN\n",
    "\n",
    "- How many of the actual positive cases did we correctly identify?\n",
    "- Useful when the cost of false negatives is high. i.e. in disease detection\n",
    "\n",
    "sklearn provides the [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) function which takes 2 parameters \n",
    "- the test labels \n",
    "- the predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, preds)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall of ~0.0003, we hardly identified any of the loan defaults, hopefully, now you are starting to understand why this model is not reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score = 2((precision * recall)/(precision + recall))\n",
    "\n",
    "- Useful when we need a balance between precision and recall\n",
    "- Less affected by large numbers of true negatives than accuracy \n",
    "\n",
    "sklearn provides the [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function which takes 2 parameters \n",
    "- the test labels \n",
    "- the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, preds)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score of ~0.0006 should prove beyond doubt that our model is not reliable despite the 78% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Roc Curve\n",
    "\n",
    "We can dig deeper into the performance of a model by plotting the ROC curve and calculating the AUC\n",
    "\n",
    "- To plot the ROC curve we first need to extract the predicted probabilities from our model against the test set\n",
    "- This can be done by passing the test features to sklearns [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logistic_model.predict_proba(x_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) is a multi-dimensional array\n",
    "- The number of columns is the number of values in the target variable \n",
    "- The number of rows is the number of samples in the test data \n",
    "\n",
    "Like a dataframe, we can use the shape property of a multi-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have 46631 rows and 2 columns \n",
    "- Cells in the first column give the predicted probability that a data point belongs to class 0\n",
    "- Cells in the second column give the predicted probability that a data point belongs to class 1\n",
    "\n",
    "Let's look at the values in these columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs[:, 0])\n",
    "print(probs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a dataframe and get the summary statistics for our predicted probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df = pd.DataFrame()\n",
    "probs_df['prob_0'] = probs[:, 0]\n",
    "probs_df['prob_1'] = probs[:, 1]\n",
    "\n",
    "probs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ROC Curve \n",
    "\n",
    "- sklearn does not provide an out of the box function for plotting the ROC curve (see scikitplot)\n",
    "- but it does provide the [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) function which gets us some of the way \n",
    "\n",
    "We pass [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) three parameters\n",
    "- the real labels for the test yet \n",
    "- the predicted probability of class 1 for the test set \n",
    "- pos_label, the class label of the positive class (1 in our case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, probs[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) function returns 3 values\n",
    "\n",
    "- threshold\n",
    "- fpr (False Positive Rate)\n",
    "- tpr (True Positive Rate)\n",
    "\n",
    "Let's take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FPR: \", fpr)\n",
    "print(\"TPR: \", tpr)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok each of these variables is an array of floating-point values, let's put them into a dataframe to try and make sense of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df = pd.DataFrame()\n",
    "roc_df['fpr'] = fpr\n",
    "roc_df['tpr'] = tpr\n",
    "roc_df['threshold'] = threshold\n",
    "\n",
    "roc_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each variable has 16454 values\n",
    "- The values of fpr and tpr range from 0 to 1\n",
    "\n",
    "Essentially, the roc_curve function is evaluating the tpr and fpr for our model using different classification thresholds \n",
    "\n",
    "- Each value in threshold represents a different classification threshold\n",
    "\n",
    "We can use these values to calculate AUC for our model \n",
    "\n",
    "The sklearn [auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) function takes two parameters\n",
    "\n",
    "- the fpr from [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "- the tpr from [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has an AUC of ~0.61\n",
    "\n",
    "- The AUC score for a random classifier is 0.5!\n",
    "- Our model is not doing a good job of separating the classes \n",
    "\n",
    "To make life easier we have provided the plot_roc_curve function \n",
    "\n",
    "- Check out the supplementary material for an explanation of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 - Advanced Evaluation\n",
    "\n",
    "In this lesson, we will dig deeper into our model performance by looking at class percentage splits and probability distributions \n",
    "\n",
    "To make life easier we will create a new DataFrame called results_df with three columns \n",
    "- true_class: the true class labels for our test set \n",
    "- predicted_class: the predicted class labels for our test set \n",
    "- default_prob: the predicted probability of default for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['true_class'] = y_test\n",
    "results_df['predicted_class'] = list(preds)\n",
    "results_df['default_prob'] = probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Prediction Percentages \n",
    "\n",
    "We can get more insight into model performance by looking at the class prediction percentages \n",
    "\n",
    "In other words \n",
    "- What percentage of our 1s were predicted as 1s (TPR)\n",
    "- What percentage of our 1s were predicted as 0s (FNR)\n",
    "- What percentage of our 0s were predicted as 0s (TNR)\n",
    "- What percentage of our 0s were predicted as 1s (FPR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "- Using [df.groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) and [value_counts](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) can you print the above-mentioned metrics\n",
    "- Feel free to look back at examples of groupby and value_counts from earlier in the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('true_class')['predicted_class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, nearly all of our test cases were classified as 0s (non defaulted loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Probability Distributions \n",
    "\n",
    "- We can use results_df to plot the class probability distributions \n",
    "\n",
    "Pandas allows us to filter rows using boolean operations\n",
    "\n",
    "For example, the code below can be used to get the predicted default probability of all the non-defaulted loans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_probs_f = results_df[results_df['true_class'] == 0]['default_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "- Use pandas filtering to create a variable default_probs_t which contains the predicted default probability for all true defaulted loans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_probs_t = results_df[results_df['true_class'] == 1]['default_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use seaborn [distplot](https://seaborn.pydata.org/generated/seaborn.distplot.html) to plot the distributions of our new variables on one chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(default_probs_f, label=\"No Default\", hist=False)\n",
    "sns.distplot(default_probs_t, label=\"Default\", hist=False)\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart again shows us that the model has very poor separability\n",
    "\n",
    "However, we can see that the probability of a defaulted loan being predicted as a default is generally higher than the same probability for a non-default so maybe there is something we can work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "We have discussed some fundamental techniques for evaluating your model, as with everything, in the real world building predictive models requires several iterations \n",
    "therefore it is a good idea to wrap up your evaluation code into a reusable function. \n",
    "\n",
    "Based on what we have discussed please fill in the function declaration below to create a reusable evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, x_test, y_test):\n",
    "    preds = model.predict(x_test)\n",
    "    probs = model.predict_proba(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    plot_confusion_matrix(model, x_test, y_test)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \", f1)\n",
    "\n",
    "    #Show ROC Curve \n",
    "    fpr, tpr, threshold = roc_curve(y_test, probs[:,1], pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC: \", roc_auc)\n",
    "\n",
    "    plot_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['true_class'] = y_test\n",
    "    results_df['predicted_class'] = list(preds)\n",
    "    results_df['default_prob'] = probs[:, 1]\n",
    "\n",
    "    #plot the distribution of probabilities for the estimated classes \n",
    "    sns.distplot(results_df[results_df['true_class'] == 0]['default_prob'], label=\"No Default\", hist=False)\n",
    "    sns.distplot(results_df[results_df['true_class'] == 1]['default_prob'], label=\"Default\", hist=False)\n",
    "    plt.show()\n",
    "    \n",
    "    #see the true class versus predicted class as a percentage\n",
    "    print(results_df.groupby('true_class')['predicted_class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(logistic_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}